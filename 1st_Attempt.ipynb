{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction Practise Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aims of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Create a function that can accurately extract the subject of a sentence \n",
    "- Consider problems with references to 'He'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Spacy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare Text Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"An eighth straight Bundesliga title has now loomed sharply into view for Bayern Munich thanks to Kimmich's chip evading Burki's desperate but doomed grasp.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into Clauses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function to split sentence into clauses based on nltk tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert string into tree\n",
    "# taken from Stack overflow post by Christos Baziotis Aug 23 '16\n",
    "# https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return node.orth_\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(docu):\n",
    "    for token in docu:\n",
    "        if token.head is token:\n",
    "            return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Spacy Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = [to_nltk_tree(sent.root) for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method with regex parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag,RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Many tourists visit the Cardiff Castle when there are tours and historical visits'\n",
    "toks = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = pos_tag(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"CHUNK: {<.*>+?<NN|NNP><.*>+?<NN|NNP>?}\" \n",
    "cp = RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = cp.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (CHUNK An/DT eighth/JJ straight/NN Bundesliga/NNP title/NN)\n",
      "  (CHUNK\n",
      "    has/VBZ\n",
      "    now/RB\n",
      "    loomed/VBN\n",
      "    sharply/RB\n",
      "    into/IN\n",
      "    view/NN\n",
      "    for/IN\n",
      "    Bayern/NNP)\n",
      "  (CHUNK Munich/NNP thanks/NNS to/TO Kimmich's/NNP chip/NN)\n",
      "  (CHUNK evading/VBG Burki's/NNP desperate/NN)\n",
      "  but/CC\n",
      "  doomed/VBD\n",
      "  grasp./NNS)\n"
     ]
    }
   ],
   "source": [
    "for subtree in parsed.subtrees():\n",
    "    if subtree.label()==\"S\" or subtree.label()==\"SBAR\":\n",
    "        print(subtree)\n",
    "        #subtexts.append(' '.join(subtree.leaves()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop to evaluate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Robert Lewandowski's first-half penalty and Benjamin Pavard's second-half header pushed Die Roten back four points clear at the top of the Bundesliga table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert --> compound --> PROPN\n",
      "Lewandowski --> poss --> PROPN\n",
      "'s --> case --> PART\n",
      "first --> amod --> ADJ\n",
      "- --> punct --> PUNCT\n",
      "half --> compound --> NOUN\n",
      "penalty --> nsubj --> NOUN\n",
      "and --> cc --> CCONJ\n",
      "Benjamin --> compound --> PROPN\n",
      "Pavard --> poss --> PROPN\n",
      "'s --> case --> PART\n",
      "second --> amod --> ADJ\n",
      "- --> punct --> PUNCT\n",
      "half --> compound --> NOUN\n",
      "header --> conj --> NOUN\n",
      "pushed --> ROOT --> VERB\n",
      "Die --> compound --> PROPN\n",
      "Roten --> dobj --> PROPN\n",
      "back --> advmod --> ADV\n",
      "four --> nummod --> NUM\n",
      "points --> npadvmod --> NOUN\n",
      "clear --> advmod --> ADJ\n",
      "at --> prep --> ADP\n",
      "the --> det --> DET\n",
      "top --> pobj --> NOUN\n",
      "of --> prep --> ADP\n",
      "the --> det --> DET\n",
      "Bundesliga --> compound --> PROPN\n",
      "table --> pobj --> NOUN\n"
     ]
    }
   ],
   "source": [
    "for tok in test_doc: \n",
    "    print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)\n",
    "    # print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eighth ORDINAL\n",
      "Bundesliga GPE\n",
      "Bayern Munich ORG\n",
      "Kimmich ORG\n",
      "Burki GPE\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to identify targets in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_target(string):\n",
    "    \n",
    "    # Declare dictionary of matches\n",
    "    sentence_dictionary = {\n",
    "        'nsubj': '',\n",
    "        'subjpass': '',\n",
    "        'pobj': '',\n",
    "        'dobj': '',\n",
    "        \"poss\": '',\n",
    "        \"children\": ''\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Iterate through tokens in input string\n",
    "    for i, tok in enumerate(string):\n",
    "        \n",
    "        \n",
    "        # Only consider proper nouns or pronouns\n",
    "        if str(tok.pos_) == \"PROPN\" or str(tok.pos_) == \"PRON\":\n",
    "            # Find nsubj\n",
    "            if str(tok.dep_) == \"nsubj\":\n",
    "                sentence_dictionary['nsubj'] += str(tok)\n",
    "                sentence_dictionary['children'] += str([token.text for token in tok.children])[1:-1]\n",
    "            # Find passive subj\n",
    "            elif str(tok.dep_) == (\"subjpass\"):\n",
    "                sentence_dictionary['subjpass'] += str(tok)\n",
    "                sentence_dictionary['children'] += str([token.text for token in tok.children])[1:-1]\n",
    "            # Find pobj    \n",
    "            elif str(tok.dep_) == (\"pobj\"):\n",
    "                sentence_dictionary['pobj'] += str(tok)\n",
    "                sentence_dictionary['children'] += str([token.text for token in tok.children])[1:-1]\n",
    "            # Find dobj \n",
    "            elif str(tok.dep_) == (\"dobj\"):\n",
    "                sentence_dictionary['dobj'] += str(tok)\n",
    "                sentence_dictionary['children'] += str([token.text for token in tok.children])[1:-1]\n",
    "            # Find possessive\n",
    "            elif str(tok.dep_) == (\"poss\"):\n",
    "                sentence_dictionary['poss'] += str(tok)\n",
    "                sentence_dictionary['children'] += str([token.text for token in tok.children])[1:-1]\n",
    "                \n",
    "    \n",
    "    # Include logic here to return a target from the dictionary\n",
    "    \n",
    "    \n",
    "    # Include logic to match children with predefined tag list\n",
    "    \n",
    "        \n",
    "    # Return the dictionary\n",
    "    return sentence_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nsubj': '',\n",
       " 'subjpass': '',\n",
       " 'pobj': 'Munich',\n",
       " 'dobj': '',\n",
       " 'poss': 'KimmichBurki',\n",
       " 'children': '\\'Bayern\\'\"\\'s\"\"\\'s\"'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_target(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
